{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c218e32",
   "metadata": {},
   "source": [
    "# Simple implementation for rectangular normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9caba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import distrax\n",
    "import haiku as hk\n",
    "\n",
    "from ima.upsampling import Pad\n",
    "\n",
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ac79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "\n",
    "d = 10\n",
    "D = 1000\n",
    "N = 10000\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "sources = jax.random.normal(subkey, shape=(N, d))\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "A = jax.random.normal(subkey, shape=(D, d)) / np.sqrt(d)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "mv = lambda m, v: jnp.matmul(m, v)\n",
    "mbv = jax.vmap(mv, (None, 0), 0)\n",
    "observations = mbv(A, sources) + 0.2 * jax.random.normal(subkey, shape=(N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89069ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Real NVP flow with Distrax\n",
    "def mk_flow(K = 16, nl = 2, hu = 256):\n",
    "    pad = Pad((0, D - d))\n",
    "    layers = []\n",
    "    for i in range(K):\n",
    "        mlp = hk.Sequential([hk.nets.MLP(nl * (hu,), activate_final=True),\n",
    "                             hk.Linear(D, w_init=jnp.zeros, b_init=jnp.zeros)])\n",
    "        def bij_fn(params):\n",
    "            bij = distrax.ScalarAffine(shift=params[..., :D // 2], log_scale=params[..., D // 2:])\n",
    "            return distrax.Block(bij, 1)\n",
    "        layers.append(distrax.SplitCoupling(D // 2, 1, mlp, bij_fn, swap=bool(i % 2)))\n",
    "    flow = distrax.Chain(layers)\n",
    "    return (pad, flow)\n",
    "\n",
    "def fwd_(x):\n",
    "    pad, flow = mk_flow()\n",
    "    \n",
    "    x = pad.forward(x)\n",
    "    return flow.forward(x)\n",
    "\n",
    "def inv_(x):\n",
    "    pad, flow = mk_flow()\n",
    "    \n",
    "    x = flow.inverse(x)\n",
    "    return pad.inverse(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f04dd59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "fwd = hk.transform(fwd_)\n",
    "inv = hk.transform(inv_)\n",
    "params = fwd.init(subkey, jnp.array(np.random.randn(5, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def loss_(args):\n",
    "    x, lam, beta = args\n",
    "    pad, flow = mk_flow()\n",
    "    \n",
    "    fwd = lambda y: flow.forward(pad.forward(y))\n",
    "    inv = lambda y: pad.inverse(flow.inverse(y))\n",
    "    \n",
    "    base_dist = distrax.Independent(distrax.Normal(loc=jnp.zeros(d), scale=jnp.ones(d)),\n",
    "                                    reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    jac_fn = jax.vmap(jax.jacfwd(fwd))\n",
    "    \n",
    "    z = inv(x)\n",
    "    jac = jac_fn(z)\n",
    "    \n",
    "    jj = jax.lax.batch_matmul(jnp.transpose(jac, (0, 2, 1)), jac)\n",
    "    chol = jax.vmap(jax.scipy.linalg.cholesky)(jj)\n",
    "    log_det = jnp.sum(jnp.log(jax.vmap(jnp.diag)(chol)), -1)\n",
    "    \n",
    "    diff = jnp.mean((x - fwd(z)) ** 2)\n",
    "    \n",
    "    return jnp.mean(-lam * (base_dist.log_prob(z) - log_det) + beta * diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "loss = hk.transform(loss_)\n",
    "params = loss.init(subkey, (jnp.array(np.random.randn(5, D)), 1., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = jnp.array(np.random.randn(5, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999be58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss.apply(params, None, (b, 1., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce75898",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1.e-3\n",
    "\n",
    "opt_init, opt_update, get_params = adam(step_size=lr)\n",
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def step(it_, opt_state_, x_, lam_, beta_):\n",
    "    params_ = get_params(opt_state_)\n",
    "    value, grads = jax.value_and_grad(loss.apply, 0)(params_, None, (x_, lam_, beta_))\n",
    "    opt_state_ = opt_update(it_, grads, opt_state_)\n",
    "    return value, opt_state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765926a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "lam_int = [40, 2000]\n",
    "batch_size = 256\n",
    "beta = 20.\n",
    "\n",
    "loss_hist = np.zeros((0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ed3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in tqdm(range(num_iter)):\n",
    "    x = observations[np.random.choice(N, batch_size)]\n",
    "    \n",
    "    # Need to warm up lambda due to stability issues\n",
    "    lam = np.interp(it, lam_int, [0, 1])\n",
    "    \n",
    "    loss_val, opt_state = step(it, opt_state, x, lam, beta)\n",
    "    \n",
    "    loss_append = np.array([[it + 1, loss_val.item()]])\n",
    "    loss_hist = np.concatenate([loss_hist, loss_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c18c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(loss_hist[:, 0], loss_hist[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cdf53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
